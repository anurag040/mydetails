<div class="analysis-matrix-container">
  <!-- Simplified Header -->
  <div class="matrix-header">
    <h2>üìä Statistical Analysis Quality Dashboard</h2>
    <p>Detailed breakdown of analysis performance and coverage</p>
  </div>

  <!-- Loading State -->
  <div class="loading-state" *ngIf="loading">
    <mat-spinner diameter="40"></mat-spinner>
    <p>Loading analysis data...</p>
  </div>

  <!-- Main Content -->
  <div class="matrix-content" *ngIf="matrix && !loading">
    
    <!-- Key Metrics Overview -->
    <div class="metrics-overview">
      <div class="metric-card">
        <h3>{{ matrix.overall_quality_score.toFixed(1) }}%</h3>
        <p>Overall Quality Score</p>
      </div>
      <div class="metric-card">
        <h3>{{ matrix.total_analyses }}</h3>
        <p>Analyses Completed</p>
      </div>
      <div class="metric-card">
        <h3>{{ getCoveragePercentage() }}%</h3>
        <p>Statistical Coverage</p>
      </div>
    </div>

    <!-- Statistical Analysis Details -->
    <div class="analysis-details-section">
      <h3>üìä Statistical Analysis Performance</h3>
      
      <div class="analysis-grid">
        <div class="analysis-card" *ngFor="let record of matrix.analysis_records; let i = index">
          <div class="card-header">
            <h4>{{ getAnalysisTypeName(record.analysis_type) }}</h4>
            <div class="quality-score" [style.color]="getScoreColor(record.score.overall_score)">
              {{ record.score.overall_score.toFixed(1) }}%
            </div>
          </div>
          
          <div class="card-content">
            <div class="analysis-metrics">
              <div class="metric">
                <span class="label">Methodology:</span>
                <span class="value" [style.color]="getScoreColor(record.score.methodology_score)">{{ record.score.methodology_score }}%</span>
              </div>
              <div class="metric">
                <span class="label">Completeness:</span>
                <span class="value" [style.color]="getScoreColor(record.score.completeness_score)">{{ record.score.completeness_score }}%</span>
              </div>
              <div class="metric">
                <span class="label">Accuracy:</span>
                <span class="value" [style.color]="getScoreColor(record.score.accuracy_score)">{{ record.score.accuracy_score }}%</span>
              </div>
            </div>
            
            <div class="analysis-summary">
              <h5>Key Findings:</h5>
              <ul>
                <li *ngFor="let summary of getAnalysisSummary(record).slice(0, 3)">{{ summary }}</li>
              </ul>
            </div>
            
            <div class="recommendations" *ngIf="record.recommendations.length > 0">
              <h5>Recommendations:</h5>
              <ul>
                <li *ngFor="let rec of record.recommendations.slice(0, 2)">{{ rec }}</li>
              </ul>
            </div>
            
            <!-- Technical Implementation Details -->
            <div class="technical-details">
              <h5>üîß Technical Implementation</h5>
              <div class="implementation-breakdown">
                <div class="methodology-detail">
                  <strong>Methodology Score ({{ record.score.methodology_score }}%):</strong>
                  <div class="method-explanation">
                    {{ getTechnicalMethodology(record.analysis_type) }}
                  </div>
                </div>
                
                <div class="completeness-detail">
                  <strong>Completeness Score ({{ record.score.completeness_score }}%):</strong>
                  <div class="completeness-explanation">
                    {{ getCompletenessCalculation(record.analysis_type) }}
                  </div>
                </div>
                
                <div class="accuracy-detail">
                  <strong>Accuracy Score ({{ record.score.accuracy_score }}%):</strong>
                  <div class="accuracy-explanation">
                    {{ getAccuracyImplementation(record.analysis_type) }}
                  </div>
                </div>
              </div>
              
              <div class="algorithm-details">
                <h6>Algorithm Implementation:</h6>
                <div class="algorithm-code">
                  <code>{{ getAlgorithmDetails(record.analysis_type) }}</code>
                </div>
              </div>
              
              <div class="validation-steps">
                <h6>Validation Process:</h6>
                <ol class="validation-list">
                  <li *ngFor="let step of getValidationSteps(record.analysis_type)">{{ step }}</li>
                </ol>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Coverage Summary -->
    <div class="coverage-summary">
      <h3>üìã Analysis Coverage</h3>
      <div class="coverage-items">
        <div class="coverage-grid">
          <div *ngFor="let item of getCoverageItems()" class="coverage-item">
            <mat-icon [class]="item.value ? 'covered' : 'not-covered'">
              {{ item.value ? 'check_circle' : 'radio_button_unchecked' }}
            </mat-icon>
            <div class="coverage-content">
              <div class="coverage-label">
                {{ item.label }}
                <span *ngIf="item.score !== undefined" class="accuracy-score" 
                      [class.excellent]="item.score >= 85"
                      [class.good]="item.score >= 70 && item.score < 85"
                      [class.fair]="item.score >= 50 && item.score < 70"
                      [class.poor]="item.score < 50">
                  {{ item.score | number:'1.0-1' }}%
                </span>
              </div>
              <div class="coverage-description">{{ item.description }}</div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Statistical Formulas and Validation Framework -->
    <div class="validation-framework">
      <h3>üî¨ Statistical Validation Framework</h3>
      
      <!-- Data Source Indicator -->
      <div class="data-source-indicator">
        <div class="source-badge" [class.mock]="isUsingMockData()" [class.real]="!isUsingMockData()">
          <mat-icon>{{ isUsingMockData() ? 'science' : 'dataset' }}</mat-icon>
          <span>{{ isUsingMockData() ? 'Mock Data (Demo)' : 'Real Analysis Data' }}</span>
        </div>
        <p *ngIf="isUsingMockData()" class="mock-warning">
          ‚ö†Ô∏è Currently displaying hypothetical data for demonstration. Load a dataset and run analyses to see real results.
        </p>
      </div>

      <!-- Core Statistical Formulas -->
      <div class="formulas-section">
        <h4>üìê Core Statistical Formulas</h4>
        
        <div class="formula-grid">
          <div class="formula-card">
            <h5>Overall Quality Score</h5>
            <div class="formula-display">
              Q = (0.35 √ó SA + 0.25 √ó AC + 0.25 √ó LC + 0.15 √ó RE) √ó 100
            </div>
            <div class="formula-explanation">
              <strong>Where:</strong><br>
              SA = Statistical Accuracy, AC = Analysis Completeness<br>
              LC = Logical Consistency, RE = Response Efficiency
            </div>
          </div>

          <div class="formula-card">
            <h5>Statistical Accuracy</h5>
            <div class="formula-display">
              SA = |{{ '{' }}correct_claims{{ '}' }}| / |{{ '{' }}total_claims{{ '}' }}|
            </div>
            <div class="formula-explanation">
              Validates statistical claims against computed results using tolerance-based matching (¬±5% for continuous variables)
            </div>
          </div>

          <div class="formula-card">
            <h5>Pearson Correlation</h5>
            <div class="formula-display">
              r = Œ£((xi - xÃÑ)(yi - »≥)) / ‚àö(Œ£(xi - xÃÑ)¬≤Œ£(yi - »≥)¬≤)
            </div>
            <div class="formula-explanation">
              Measures linear relationship strength between variables. |r| > 0.7 indicates strong correlation.
            </div>
          </div>

          <div class="formula-card">
            <h5>Shapiro-Wilk Test</h5>
            <div class="formula-display">
              W = (Œ£ai x(i))¬≤ / Œ£(xi - xÃÑ)¬≤
            </div>
            <div class="formula-explanation">
              Tests normality assumption. W > 0.95 suggests normal distribution (p > 0.05).
            </div>
          </div>

          <div class="formula-card">
            <h5>Principal Component Analysis</h5>
            <div class="formula-display">
              PC‚ÇÅ = a‚ÇÅ‚ÇÅX‚ÇÅ + a‚ÇÅ‚ÇÇX‚ÇÇ + ... + a‚ÇÅ‚ÇöX‚Çö
            </div>
            <div class="formula-explanation">
              First principal component captures maximum variance. Eigenvalues > 1.0 indicate significant components.
            </div>
          </div>

          <div class="formula-card">
            <h5>Chi-Square Test</h5>
            <div class="formula-display">
              œá¬≤ = Œ£((Oi - Ei)¬≤ / Ei)
            </div>
            <div class="formula-explanation">
              Tests independence in categorical data. p-value > 0.05 suggests independence.
            </div>
          </div>
        </div>
      </div>

      <!-- Detailed Validation Metrics -->
      <div class="validation-metrics">
        <h4>üéØ Validation Metrics Breakdown</h4>
        <p class="metric-defs">
          Definitions:
          <br>- Statistical Accuracy: Proportion of LLM numerical/correlation claims that match computed ground truth within tolerance.
          <br>- Analysis Completeness: Coverage across domains (descriptive stats, data quality, relationships, visualization, modeling).
          <br>- Logical Consistency: 1 ‚àí (contradictions detected / checks) based on contradictory term proximity.
          <br>- Response Efficiency: Tiered score from response_time (Excellent <2s, Good 2‚Äì5s, Acceptable 5‚Äì10s, Poor >10s).
        </p>
        
        <div class="metrics-detailed">
      <div class="metric-detail-card">
            <div class="metric-header">
              <mat-icon>analytics</mat-icon>
              <h5>Statistical Accuracy (35% Weight)</h5>
            </div>
            <div class="metric-content">
              <div class="metric-score">{{ hasLLMValidation() ? (getStatisticalAccuracy() + '%') : 'N/A' }}</div>
              <div class="metric-components">
                <div class="component">
                  <span class="label">Computational Precision:</span>
          <span class="value">{{ llmValidation?.statistical_accuracy?.details?.computational_precision != null ? (llmValidation.statistical_accuracy.details.computational_precision * 100 | number:'1.0-1') + '%' : 'N/A' }}</span>
                </div>
                <div class="component">
                  <span class="label">Statistical Test Validity:</span>
          <span class="value">{{ llmValidation?.statistical_accuracy?.details?.test_validity != null ? (llmValidation.statistical_accuracy.details.test_validity * 100 | number:'1.0-1') + '%' : 'N/A' }}</span>
                </div>
                <div class="component">
                  <span class="label">Assumption Verification:</span>
          <span class="value">{{ llmValidation?.statistical_accuracy?.details?.assumption_verification != null ? (llmValidation.statistical_accuracy.details.assumption_verification * 100 | number:'1.0-1') + '%' : 'N/A' }}</span>
                </div>
              </div>
              <div class="implementation-details">
                <h6>Implementation:</h6>
                <ul>
                  <li>Tolerance thresholds: ¬±5% for continuous, exact for categorical</li>
                  <li>Cross-validation against R/Python statistical libraries</li>
                  <li>Floating-point precision handling with IEEE 754 standards</li>
                </ul>
              </div>
              <div class="justification-section">
                <h6>Justification & Procedure:</h6>
                <div class="justification-content">
                  <p><strong>Why 35% Weight:</strong> Statistical accuracy is the most critical factor as incorrect calculations invalidate all subsequent interpretations and decisions.</p>
                  <p><strong>Validation Process:</strong></p>
                  <ol>
                    <li>Extract numerical claims from LLM responses using regex patterns</li>
                    <li>Compute ground truth values using validated statistical libraries (scipy, numpy)</li>
                    <li>Compare claims against ground truth with tolerance-based matching</li>
                    <li>Account for floating-point precision limitations and rounding conventions</li>
                    <li>Score based on proportion of accurate claims: (correct_claims / total_claims) √ó 100</li>
                  </ol>
                  <p><strong>Quality Thresholds:</strong> >95% Excellent, 85-95% Good, 70-85% Acceptable, <70% Poor</p>
                </div>
              </div>
            </div>
          </div>

          <div class="metric-detail-card">
            <div class="metric-header">
              <mat-icon>checklist</mat-icon>
              <h5>Analysis Completeness (25% Weight)</h5>
            </div>
            <div class="metric-content">
              <div class="metric-score">{{ hasLLMValidation() ? (getCompletenessScore() + '%') : 'N/A' }}</div>
              <div class="domain-coverage">
                <h6>Domain Coverage:</h6>
                <div class="domain-item">
                  <mat-icon>check</mat-icon>
                  <span>Descriptive Statistics: Mean, median, mode, std dev, skewness, kurtosis</span>
                </div>
                <div class="domain-item">
                  <mat-icon>check</mat-icon>
                  <span>Distribution Analysis: Normality tests, Q-Q plots, histograms</span>
                </div>
                <div class="domain-item">
                  <mat-icon>check</mat-icon>
                  <span>Correlation Analysis: Pearson, Spearman, partial correlations</span>
                </div>
                <div class="domain-item">
                  <mat-icon>check</mat-icon>
                  <span>Data Quality: Missing values, outliers, duplicates detection</span>
                </div>
                <div class="domain-item">
                  <mat-icon>check</mat-icon>
                  <span>Advanced Analytics: PCA, clustering, regression diagnostics</span>
                </div>
              </div>
              <div class="justification-section">
                <h6>Justification & Procedure:</h6>
                <div class="justification-content">
                  <p><strong>Why 25% Weight:</strong> Comprehensive coverage ensures no critical analytical blind spots that could lead to incomplete insights or missed patterns.</p>
                  <p><strong>Assessment Process:</strong></p>
                  <ol>
                    <li>Define 17 core analysis domains (descriptive, correlation, distribution, etc.)</li>
                    <li>Check presence of each domain in analysis output using keyword matching</li>
                    <li>Verify depth of coverage within each domain (basic vs comprehensive)</li>
                    <li>Score based on weighted coverage: (covered_domains / total_domains) √ó domain_depth_factor</li>
                    <li>Apply domain importance weights (descriptive stats: 20%, data quality: 25%, etc.)</li>
                  </ol>
                  <p><strong>Coverage Standards:</strong> All 17 domains expected for 100% score, minimum 12 domains for acceptable (70%)</p>
                </div>
              </div>
            </div>
          </div>

          <div class="metric-detail-card">
            <div class="metric-header">
              <mat-icon>verified</mat-icon>
              <h5>Logical Consistency (25% Weight)</h5>
            </div>
            <div class="metric-content">
              <div class="metric-score">{{ hasLLMValidation() ? getConsistencyScoreFormatted() : 'N/A' }}</div>
              <div class="consistency-checks">
                <h6>Consistency Validation:</h6>
                <div class="check-item">
                  <mat-icon>psychology</mat-icon>
                  <span>Contradiction Detection: Opposing claims analysis</span>
                </div>
                <div class="check-item">
                  <mat-icon>fact_check</mat-icon>
                  <span>Statistical Alignment: Claims vs computed results</span>
                </div>
                <div class="check-item">
                  <mat-icon>auto_awesome</mat-icon>
                  <span>Cross-Domain Consistency: Interpretation coherence</span>
                </div>
              </div>
              <div class="consistency-formula">
                <strong>Formula:</strong> Consistency = 1 - (contradictions_detected / total_checks)
              </div>
              <div class="justification-section">
                <h6>Justification & Procedure:</h6>
                <div class="justification-content">
                  <p><strong>Why 25% Weight:</strong> Logical consistency prevents contradictory conclusions that undermine analytical credibility and decision-making confidence.</p>
                  <p><strong>Validation Process:</strong></p>
                  <ol>
                    <li>Parse analysis text into semantic chunks using NLP tokenization</li>
                    <li>Identify contradictory term pairs (e.g., "high correlation" vs "weak relationship")</li>
                    <li>Check statistical claim alignment with computed results (¬±2œÉ tolerance)</li>
                    <li>Verify cross-domain consistency (e.g., outlier detection vs distribution analysis)</li>
                    <li>Score: 1 - (contradiction_count / total_consistency_checks) √ó 100</li>
                  </ol>
                  <p><strong>Consistency Benchmarks:</strong> >95% Excellent, 90-95% Good, 80-90% Acceptable, <80% Poor</p>
                </div>
              </div>
            </div>
          </div>

          <div class="metric-detail-card">
            <div class="metric-header">
              <mat-icon>speed</mat-icon>
              <h5>Response Efficiency (15% Weight)</h5>
            </div>
            <div class="metric-content">
              <div class="metric-score">{{ hasLLMValidation() ? getEfficiencyScoreFormatted() : 'N/A' }}</div>
              <div class="efficiency-tiers">
                <div class="tier excellent">
                  <span class="tier-name">Excellent (&lt;2s):</span>
                  <span class="tier-score">100%</span>
                </div>
                <div class="tier good">
                  <span class="tier-name">Good (2-5s):</span>
                  <span class="tier-score">85%</span>
                </div>
                <div class="tier acceptable">
                  <span class="tier-name">Acceptable (5-10s):</span>
                  <span class="tier-score">70%</span>
                </div>
                <div class="tier poor">
                  <span class="tier-name">Poor (&gt;10s):</span>
                  <span class="tier-score">50%</span>
                </div>
              </div>
              <div class="current-performance" *ngIf="hasLLMValidation(); else noEff">
                <strong>Current:</strong> {{ getResponseTime() | number:'1.1-1' }}s ({{ getActualEfficiencyTier() }})
              </div>
              <ng-template #noEff>
                <div class="current-performance">
                  <strong>Current:</strong> N/A
                </div>
              </ng-template>
              <div class="justification-section">
                <h6>Justification & Procedure:</h6>
                <div class="justification-content">
                  <p><strong>Why 15% Weight:</strong> Response efficiency impacts user experience and system scalability, but is less critical than analytical accuracy and completeness.</p>
                  <p><strong>Measurement Process:</strong></p>
                  <ol>
                    <li>Record timestamp at analysis request initiation</li>
                    <li>Capture completion timestamp when final results are returned</li>
                    <li>Calculate total response time: completion_time - start_time</li>
                    <li>Apply tiered scoring based on response time ranges</li>
                    <li>Account for analysis complexity and dataset size in scoring adjustments</li>
                  </ol>
                  <p><strong>Performance Standards:</strong> <2s Excellent (100%), 2-5s Good (85%), 5-10s Acceptable (70%), >10s Poor (50%)</p>
                  <p><strong>Complexity Adjustments:</strong> +50% time allowance for datasets >10MB, +25% for >15 analysis types</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
